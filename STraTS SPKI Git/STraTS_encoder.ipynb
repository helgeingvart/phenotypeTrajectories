{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52aa6a2",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae59487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to be the same in strats_notebook.ipynb and clutering_STraTS_encoder.ipynb \n",
    "HYPER_PARAM = {'d_var':40, 'd_demo':10, 'N':2, 'he':4, 'dropout':0.2}\n",
    "DATA_SET = 'bodo' # can be {'bodo', 'physionet'}\n",
    "MIN_LENGTH_OF_STAY = 8\n",
    "MAX_LENGTH_OF_STAY = 60\n",
    "SAMPLES_TO_PREDICT = 1\n",
    "WARD_TO_USE = 'WardALL'                             # alternatives {'Ward2','Ward3','WardALL','WardNO'}\n",
    "STATIC_VARIS = ['Gender', 'StaticImputed', 'WardID', 'WardChange']          # alternatives ['Gender', 'StaticImputed', 'WardChange', 'Unconscious', 'WardID'  + in physionet 'weight', 'Age']\n",
    "DROP_VAIRS = []\n",
    "USE_DAY_MEAN = ''       # '', '_dayMean'  take mean of values for each day.\n",
    "IMPUTE_GENDER = '_ImputeGender' # '_ImputeGender'  impute gender with random gender with same rate as data.\n",
    "REMOVE_MISSING_GENDER = '' # '_noMissGender'  remove patient if gender is missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc5b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique for this file:\n",
    "DATA_PATH = './data/'\n",
    "DATA_NAME = f'triplets_{DATA_SET}_O{MIN_LENGTH_OF_STAY}U{MAX_LENGTH_OF_STAY}_{WARD_TO_USE}{IMPUTE_GENDER}{REMOVE_MISSING_GENDER}{USE_DAY_MEAN}.pkl'\n",
    "SAVE_WEIGHTS_AS = f'weight_{DATA_SET}_O{MIN_LENGTH_OF_STAY}U{MAX_LENGTH_OF_STAY}_{WARD_TO_USE}_SV{STATIC_VARIS}_pred{SAMPLES_TO_PREDICT}{IMPUTE_GENDER}{REMOVE_MISSING_GENDER}{USE_DAY_MEAN}_d_var{HYPER_PARAM[\"d_var\"]}demo{HYPER_PARAM[\"d_demo\"]}.h5'\n",
    "\n",
    "# STATIC_VARIS = []\n",
    "\n",
    "SAVE_WEIGHTS_AS\n",
    "print(f\"Save weigts as: {SAVE_WEIGHTS_AS}\")\n",
    "print(\"NOTE: Need to manually move weights to the <weigts> folder to use them further in the clustering_STraTS_encoder script. \\nThis is to make sure that you don't overwrite them.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.config.experimental import list_physical_devices\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "# making tf_utils.smart_cond work with latest version of tensorflow 2.xx\n",
    "\n",
    "from tensorflow.python.framework import smart_cond as smart_module\n",
    "from tensorflow.python.ops import control_flow_ops, variables\n",
    "import time\n",
    "\n",
    "\n",
    "def smart_cond(pred, true_fn=None, false_fn=None, name=None):\n",
    "  if isinstance(pred, variables.Variable):\n",
    "    return control_flow_ops.cond(\n",
    "        pred, true_fn=true_fn, false_fn=false_fn, name=name)\n",
    "  return smart_module.smart_cond(\n",
    "      pred, true_fn=true_fn, false_fn=false_fn, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce6871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for GPU\n",
    "print(\"Num GPUs Available: \", len(list_physical_devices('GPU')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "recreational-record",
   "metadata": {},
   "source": [
    "## Load forecast dataset into matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-annual",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read data.\n",
    "if DATA_SET=='physionet':\n",
    "    data, oc, train_ind, valid_ind, test_ind = pickle.load(open(DATA_PATH+'physionet_2012_preprocessed_noNormalization.pkl', 'rb'))\n",
    "\n",
    "    static_varis = STATIC_VARIS \n",
    "\n",
    "    feat_dynamic = [\n",
    "        'HR',           # [Heart rate (bpm)]\n",
    "        'NIDiasABP',    # [Non-invasive diastolic arterial blood pressure (mmHg)]\n",
    "        'NISysABP',     # [Non-invasive systolic arterial blood pressure (mmHg)]\n",
    "        'PaO2',         # [Partial pressure of arterial O2 (mmHg)]\n",
    "        'RespRate',     # [Respiration rate (bpm)]\n",
    "        'SaO2',         # [O2 saturation in hemoglobin (%)]\n",
    "        'Temp',         # [Temperature (°C)]\n",
    "        # Features not in Bodo data below.\n",
    "        # 'Albumin',      # (g/dL)\n",
    "        # 'ALP',          # [Alkaline phosphatase (IU/L)]\n",
    "        # 'ALT',          # [Alanine transaminase (IU/L)]\n",
    "        # 'AST',          # [Aspartate transaminase (IU/L)]\n",
    "        # 'Bilirubin',    # (mg/dL)\n",
    "        # 'BUN',          # [Blood urea nitrogen (mg/dL)]\n",
    "        # 'Cholesterol',  # (mg/dL)\n",
    "        # 'Creatinine',   # [Serum creatinine (mg/dL)]\n",
    "        # 'DiasABP',      # [Invasive diastolic arterial blood pressure (mmHg)]\n",
    "        # 'FiO2',         # [Fractional inspired O2 (0-1)]\n",
    "        # 'GCS',          # [Glasgow Coma Score (3-15)]\n",
    "        # 'Glucose',      # [Serum glucose (mg/dL)]\n",
    "        # 'HCO3',         # [Serum bicarbonate (mmol/L)]\n",
    "        # 'HCT',          # [Hematocrit (%)]\n",
    "        # 'K',            # [Serum potassium (mEq/L)]\n",
    "        # 'Lactate',      # (mmol/L)\n",
    "        # 'Mg',           # [Serum magnesium (mmol/L)]\n",
    "        # 'MAP',          # [Invasive mean arterial blood pressure (mmHg)]\n",
    "        # 'MechVent',     # [Mechanical ventilation respiration (0:false, or 1:true)]\n",
    "        # 'Na',           # [Serum sodium (mEq/L)]\n",
    "        # 'NIMAP',        # [Non-invasive mean arterial blood pressure (mmHg)]\n",
    "        # 'PaCO2',        # [partial pressure of arterial CO2 (mmHg)]\n",
    "        # 'pH',           # [Arterial pH (0-14)]\n",
    "        # 'Platelets',    # (cells/nL)\n",
    "        # 'SysABP',       # [Invasive systolic arterial blood pressure (mmHg)]\n",
    "        # 'TropI',        # [Troponin-I (μg/L)]\n",
    "        # 'TropT',        # [Troponin-T (μg/L)]\n",
    "        # 'Urine',        # [Urine output (mL)]\n",
    "        # 'WBC'           # [White blood cell count (cells/nL)]\n",
    "        ]\n",
    "\n",
    "    data = data.loc[data.variable.isin(feat_dynamic)]\n",
    "\n",
    "    \n",
    "\n",
    "    # max_hour = int(data.hour.max())\n",
    "\n",
    "    MIN = MIN_LENGTH_OF_STAY\n",
    "    MAX = MAX_LENGTH_OF_STAY\n",
    "    if USE_DAY_MEAN == '_dayMean':  # should be 4 samples per day \n",
    "        MIN = MIN_LENGTH_OF_STAY // 4 \n",
    "        MAX = MAX_LENGTH_OF_STAY // 4\n",
    "    obs_windows = range(MIN-10, MAX, 10)\n",
    "\n",
    "elif DATA_SET=='bodo':\n",
    "    datapath = DATA_PATH+DATA_NAME\n",
    "    data, oc, train_ind, valid_ind, test_ind = pickle.load(open(DATA_PATH+DATA_NAME, 'rb'))\n",
    "\n",
    "    static_varis = STATIC_VARIS # ['Gender']\n",
    "    # max_hour = int(data.checkup.max())\n",
    "    MIN = MIN_LENGTH_OF_STAY\n",
    "    MAX = MAX_LENGTH_OF_STAY\n",
    "    if USE_DAY_MEAN == '_dayMean':  # should be 4 samples per day \n",
    "        print('USE_DAY_MEAN is on')\n",
    "        MIN = MIN_LENGTH_OF_STAY // 4 \n",
    "        MAX = MAX // 4\n",
    "    obs_windows = range(MIN, MAX,  1)\n",
    "    \n",
    "\n",
    "else:\n",
    "    print('cant find data, specify: DATA_PATH and DATA_NAME')\n",
    "\n",
    "print('Unsing data from:', datapath)\n",
    "print('patients in data:', len(data.PatientID.unique()))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd58791",
   "metadata": {},
   "source": [
    "# Preprocess data into forcast chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove test patients.\n",
    "test_sub = oc.loc[oc.ts_ind.isin(test_ind)].ts_ind.unique()\n",
    "data = data.loc[~data.ts_ind.isin(test_sub)]\n",
    "oc = oc.loc[~oc.ts_ind.isin(test_sub)]\n",
    "\n",
    "# Remove unwanted variables.\n",
    "data = data[~data['variable'].isin(DROP_VAIRS)]\n",
    "\n",
    "# Fix age.\n",
    "data.loc[(data.variable=='Age')&(data.value>200), 'value'] = 91.4\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "N = data.ts_ind.max()+1\n",
    "demo = np.zeros((N, D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds==0)*1 + (stds!=0)*stds\n",
    "demo = (demo-means)/stds\n",
    "\n",
    "# Get variable indices.\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'checkup', 'value']].sort_values(by=['ts_ind', 'vind', 'checkup'])\n",
    "# Find max_len.\n",
    "# fore_max_len = 880 # original\n",
    "fore_max_len = MAX_LENGTH_OF_STAY # No need for longer than max amount of datapoints for each patient?\n",
    "# Get forecast inputs and outputs.\n",
    "fore_times_ip = []\n",
    "fore_values_ip = []\n",
    "fore_varis_ip = []\n",
    "fore_op = []\n",
    "fore_inds = []\n",
    "def f(x):\n",
    "    mask = [0 for i in range(V)]\n",
    "    values = [0 for i in range(V)]\n",
    "    for vv in x:\n",
    "        v = int(vv[0])-1\n",
    "        mask[v] = 1\n",
    "        values[v] = vv[1]\n",
    "    return values+mask\n",
    "def pad(x):\n",
    "    return x+[0]*(fore_max_len-len(x))\n",
    "for w in tqdm(obs_windows):\n",
    "    # predicted data: from timestep w and SAMPLES_TO_PREDICT forward.\n",
    "    pred_data = data.loc[(data.checkup>=w)&(data.checkup<w+SAMPLES_TO_PREDICT)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value':'first'}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value':list}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data['vind_value'].apply(f)   \n",
    "    \n",
    "    # observed data: from timestep w and 24h before that.  \n",
    "    # obs_data = data.loc[(data.hour<w)&(data.hour>=w-24)] # Original STraTS only uses data within a set timeframe,\n",
    "    obs_data = data.loc[(data.checkup<w)]#&(data.hour>=w-24)] # We are using all previous data since it is so sparce anyway.\n",
    "\n",
    "    obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "    obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "    obs_data = obs_data.groupby('ts_ind').agg({'vind':list, 'checkup':list, 'value':list}).reset_index()\n",
    "    obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "    for col in ['vind', 'checkup', 'value']:\n",
    "        obs_data[col] = obs_data[col].apply(pad)\n",
    "    fore_op.append(np.array(list(obs_data.vind_value)))\n",
    "    fore_inds.append(np.array(list(obs_data.ts_ind)))\n",
    "    fore_times_ip.append(np.array(list(obs_data.checkup)))\n",
    "    fore_values_ip.append(np.array(list(obs_data.value)))\n",
    "    fore_varis_ip.append(np.array(list(obs_data.vind)))\n",
    "\n",
    "    \n",
    "del data\n",
    "fore_times_ip = np.concatenate(fore_times_ip, axis=0)\n",
    "fore_values_ip = np.concatenate(fore_values_ip, axis=0)\n",
    "fore_varis_ip = np.concatenate(fore_varis_ip, axis=0)\n",
    "fore_op = np.concatenate(fore_op, axis=0)\n",
    "fore_inds = np.concatenate(fore_inds, axis=0)\n",
    "fore_demo = demo[fore_inds]\n",
    "# Get train and valid ts_ind for forecast task.\n",
    "train_sub = oc.loc[oc.ts_ind.isin(train_ind)].ts_ind.unique()\n",
    "valid_sub = oc.loc[oc.ts_ind.isin(valid_ind)].ts_ind.unique()\n",
    "rem_sub = oc.loc[~oc.ts_ind.isin(np.concatenate((train_ind, valid_ind)))].ts_ind.unique() \n",
    "# rem_sub: remaining sub gets added to training and validation subs. \n",
    "bp = int(0.8*len(rem_sub))\n",
    "train_sub = np.concatenate((train_sub, rem_sub[:bp]))\n",
    "valid_sub = np.concatenate((valid_sub, rem_sub[bp:]))\n",
    "train_ind = oc.loc[oc.ts_ind.isin(train_sub)].ts_ind.unique() # Add remaining ts_ind s of train subjects.\n",
    "valid_ind = oc.loc[oc.ts_ind.isin(valid_sub)].ts_ind.unique() # Add remaining ts_ind s of train subjects.\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ind = np.argwhere(np.in1d(fore_inds, train_ind)).flatten()\n",
    "valid_ind = np.argwhere(np.in1d(fore_inds, valid_ind)).flatten()\n",
    "fore_train_ip = [ip[train_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "fore_valid_ip = [ip[valid_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "# del fore_times_ip, fore_values_ip, fore_varis_ip, demo, fore_demo\n",
    "fore_train_op = fore_op[train_ind]\n",
    "fore_valid_op = fore_op[valid_ind]\n",
    "del fore_op\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "imperial-concern",
   "metadata": {},
   "source": [
    "## Load target dataset into matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read data.\n",
    "if DATA_SET=='physionet':\n",
    "    data, oc, train_ind, valid_ind, test_ind = pickle.load(open(DATA_PATH+'physionet_2012_preprocessed_noNormalization.pkl', 'rb'))\n",
    "    data = data.loc[data.variable.isin(feat_dynamic)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "elif DATA_SET=='bodo':\n",
    "    data, oc, train_ind, valid_ind, test_ind = pickle.load(open(DATA_PATH+DATA_NAME, 'rb'))\n",
    "\n",
    "else:\n",
    "    print('cant find data, specify: DATA_PATH and DATA_NAME')\n",
    "\n",
    "print('Data in use: ', DATA_PATH+DATA_NAME)\n",
    "\n",
    "\n",
    "# Filter labeled data in first 24h.\n",
    "data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "data = data.loc[(data.hour>=0)&(data.hour<=24)]\n",
    "if 'StaticImputed' not in static_varis:\n",
    "        try:\n",
    "            data = data[data['variable'] != 'StaticImputed']\n",
    "            print('Dropped StaticImputed')\n",
    "        except:\n",
    "            print('No StaticImputed in data')\n",
    "oc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# Fix age.\n",
    "data.loc[(data.variable=='Age')&(data.value>200), 'value'] = 91.4\n",
    "# Get y and N.\n",
    "y = np.array(oc.sort_values(by='ts_ind')['in_hospital_mortality']).astype('float32')\n",
    "N = data.ts_ind.max() + 1\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "demo = np.zeros((N, D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds==0)*1 + (stds!=0)*stds\n",
    "demo = (demo-means)/stds\n",
    "\n",
    "\n",
    "if DATA_SET=='physionet':\n",
    "    # normalize with ideal values, Bodo features\n",
    "    # opt_val = [['PatientID','WardID','Timestamp', 'Systolic', 'Diastolic', 'O2', 'Pulse', 'Temp', 'Rf', 'Gender', 'onO2',   'Unconscious', 'EwsScore', 'StaticImputed']]\n",
    "    # opt_val = [     0,          0,          0,          120,        80,      96,    70,     37,     16,     0,        0,             0,       0,              0 ]\n",
    "\n",
    "    data.loc[data.variable == 'HR', 'value'] -= 70\n",
    "    data.loc[data.variable == 'RespRate', 'value'] -= 16\n",
    "    data.loc[data.variable == 'Temp', 'value'] -= 37\n",
    "    data.loc[data.variable == 'SaO2', 'value'] -= 96\n",
    "    data.loc[data.variable == 'NIDiasABP', 'value'] -= 80\n",
    "    data.loc[data.variable == 'NISysABP', 'value'] -= 120\n",
    "\n",
    "    # Add columns to the data: checkup, number of checkups on the patient at the given row\n",
    "    patient_ids = data.ts_ind.unique()\n",
    "    data_grouped = data.groupby('ts_ind')\n",
    "    def generate_checkups(patient_data):\n",
    "        reps = patient_data['hour'].value_counts(sort=False)\n",
    "        checkups = np.repeat(np.arange(len(reps)), reps)\n",
    "        return pd.DataFrame({'checkup': checkups})\n",
    "\n",
    "    df_checkup = data_grouped.apply(generate_checkups).reset_index(level=1, drop=True).reset_index()\n",
    "    data['checkup'] = df_checkup['checkup'].values\n",
    "\n",
    "    # Add columns to the data: PatientID, same as ts_ind for Physionet\n",
    "    data['PatientID'] = data['ts_ind']\n",
    "    data = data[['ts_ind', 'PatientID', 'checkup', 'hour', 'variable', 'value', 'mean', 'std']]\n",
    "\n",
    "    # Drop patients with less than MIN_LENGTH_OF_STAY different times with datasamples\n",
    "    droppedID = data[data_grouped['checkup'].transform('max') <= MIN_LENGTH_OF_STAY]['ts_ind'].unique()\n",
    "    data = data[~data['ts_ind'].isin(droppedID)]\n",
    "    print(f'Dropped {len(droppedID)} patients with less then MIN_LENGTH_OF_STAY:{MIN_LENGTH_OF_STAY} samples.')\n",
    "\n",
    "\n",
    "# Trim to max len.\n",
    "data = data.sample(frac=1)\n",
    "data = data.groupby('ts_ind').head(880)\n",
    "# Get N, V, var_to_ind.\n",
    "N = data.ts_ind.max() + 1\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Add obs index.\n",
    "data = data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "data = data.reset_index().rename(columns={'index':'obs_ind'})\n",
    "data = data.merge(data.groupby('ts_ind').agg({'obs_ind':'min'}).reset_index().rename(columns={ \\\n",
    "                                                            'obs_ind':'first_obs_ind'}), on='ts_ind')\n",
    "data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "# Find max_len.\n",
    "max_len = data.obs_ind.max()+1\n",
    "# print ('max_len', max_len)\n",
    "# Generate times_ip and values_ip matrices.\n",
    "times_inp = np.zeros((N, max_len), dtype='float32')\n",
    "values_inp = np.zeros((N, max_len), dtype='float32')\n",
    "varis_inp = np.zeros((N, max_len), dtype='int32')\n",
    "for row in tqdm(data.itertuples()):\n",
    "    ts_ind = row.ts_ind\n",
    "    l = row.obs_ind\n",
    "    times_inp[ts_ind, l] = row.hour\n",
    "    values_inp[ts_ind, l] = row.value\n",
    "    varis_inp[ts_ind, l] = row.vind\n",
    "data.drop(columns=['obs_ind', 'first_obs_ind'], inplace=True)\n",
    "\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ip = [ip[train_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "valid_ip = [ip[valid_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "test_ip = [ip[test_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "del times_inp, values_inp, varis_inp\n",
    "train_op = y[train_ind]\n",
    "valid_op = y[valid_ind]\n",
    "test_op = y[test_ind]\n",
    "del y\n",
    "\n",
    "# y = np.concatenate((train_op,valid_op,test_op))\n",
    "# y.sum()/len(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "convinced-vault",
   "metadata": {},
   "source": [
    "## Define metrics and losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res(y_true, y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    minrp = np.minimum(precision, recall).max()\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    return [roc_auc, pr_auc, minrp]\n",
    "    \n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=[0,1], y=train_op)\n",
    "def mortality_loss(y_true, y_pred):\n",
    "    sample_weights = (1-y_true)*class_weights[0] + y_true*class_weights[1]\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    return K.mean(sample_weights*bce, axis=-1)\n",
    "\n",
    "def forecast_loss(y_true, y_pred):\n",
    "    return K.sum(y_true[:,V:]*(y_true[:,:V]-y_pred)**2, axis=-1)\n",
    "\n",
    "def get_min_loss(weight):\n",
    "    def min_loss(y_true, y_pred):\n",
    "        return weight*y_pred\n",
    "    return min_loss\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, validation_data, batch_size):\n",
    "        self.val_x, self.val_y = validation_data\n",
    "        self.batch_size = batch_size\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.val_x, verbose=0, batch_size=self.batch_size)\n",
    "        if type(y_pred)==type([]):\n",
    "            y_pred = y_pred[0]\n",
    "        precision, recall, thresholds = precision_recall_curve(self.val_y, y_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        roc_auc = roc_auc_score(self.val_y, y_pred)\n",
    "        logs['custom_metric'] = pr_auc + roc_auc\n",
    "        print ('val_aucs:', pr_auc, roc_auc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "metallic-forth",
   "metadata": {},
   "source": [
    "## Define model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Activation, Dropout, Softmax, Layer, InputSpec, Input, Dense, Lambda, TimeDistributed, Concatenate, Add\n",
    "from tensorflow.keras import initializers, regularizers, constraints, Model\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow import nn\n",
    "\n",
    "    \n",
    "class CVE(Layer):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        super(CVE, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape): \n",
    "        self.W1 = self.add_weight(name='CVE_W1',\n",
    "                            shape=(1, self.hid_units),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        self.b1 = self.add_weight(name='CVE_b1',\n",
    "                            shape=(self.hid_units,),\n",
    "                            initializer='zeros',\n",
    "                            trainable=True)\n",
    "        self.W2 = self.add_weight(name='CVE_W2',\n",
    "                            shape=(self.hid_units, self.output_dim),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        super(CVE, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = K.expand_dims(x, axis=-1)\n",
    "        x = K.dot(K.tanh(K.bias_add(K.dot(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape + (self.output_dim,)\n",
    "    \n",
    "    \n",
    "class Attention(Layer):\n",
    "    \n",
    "    def __init__(self, hid_dim):\n",
    "        self.hid_dim = hid_dim\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        self.W = self.add_weight(shape=(d, self.hid_dim), name='Att_W',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.hid_dim,), name='Att_b',\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(shape=(self.hid_dim,1), name='Att_u',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = K.dot(K.tanh(K.bias_add(K.dot(x,self.W), self.b)), self.u)\n",
    "        mask = K.expand_dims(mask, axis=-1)\n",
    "        attn_weights = mask*attn_weights + (1-mask)*mask_value\n",
    "        attn_weights = K.softmax(attn_weights, axis=-2)\n",
    "        return attn_weights\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (1,)\n",
    "    \n",
    "    \n",
    "class Transformer(Layer):\n",
    "    \n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0):\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = K.epsilon() * K.epsilon()\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        if self.dk==None:\n",
    "            self.dk = d//self.h\n",
    "        if self.dv==None:\n",
    "            self.dv = d//self.h\n",
    "        if self.dff==None:\n",
    "            self.dff = 2*d\n",
    "        self.Wq = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wq',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wk = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wk',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wv = self.add_weight(shape=(self.N, self.h, d, self.dv), name='Wv',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wo = self.add_weight(shape=(self.N, self.dv*self.h, d), name='Wo',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.W1 = self.add_weight(shape=(self.N, d, self.dff), name='W1',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b1 = self.add_weight(shape=(self.N, self.dff), name='b1',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.W2 = self.add_weight(shape=(self.N, self.dff, d), name='W2',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b2 = self.add_weight(shape=(self.N, d), name='b2',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.gamma = self.add_weight(shape=(2*self.N,), name='gamma',\n",
    "                                 initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(shape=(2*self.N,), name='beta',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Transformer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e-30):\n",
    "        start = time.time()\n",
    "        mask = K.expand_dims(mask, axis=-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = K.dot(x, self.Wq[i,j,:,:])\n",
    "                k = K.permute_dimensions(K.dot(x, self.Wk[i,j,:,:]), (0,2,1))\n",
    "                v = K.dot(x, self.Wv[i,j,:,:])\n",
    "                A = K.batch_dot(q,k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask*A + (1-mask)*mask_value\n",
    "                # Mask for attention dropout.\n",
    "                def dropped_A():\n",
    "                    dp_mask = K.cast((K.random_uniform(shape=array_ops.shape(A))>=self.dropout), K.floatx())\n",
    "                    return A*dp_mask + (1-dp_mask)*mask_value\n",
    "                A = smart_cond(K.learning_phase(), dropped_A, lambda: array_ops.identity(A))\n",
    "                A = K.softmax(A, axis=-1)\n",
    "                mha_ops.append(K.batch_dot(A,v))\n",
    "            conc = K.concatenate(mha_ops, axis=-1)\n",
    "            proj = K.dot(conc, self.Wo[i,:,:])\n",
    "            # Dropout.\n",
    "            proj = smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(proj, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(proj))\n",
    "            # Add & LN\n",
    "            x = x+proj\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i] + self.beta[2*i]\n",
    "            # FFN\n",
    "            ffn_op = K.bias_add(K.dot(K.relu(K.bias_add(K.dot(x, self.W1[i,:,:]), self.b1[i,:])), \n",
    "                           self.W2[i,:,:]), self.b2[i,:,])\n",
    "            # Dropout.\n",
    "            ffn_op = smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(ffn_op, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(ffn_op))\n",
    "            # Add & LN\n",
    "            x = x+ffn_op\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i+1] + self.beta[2*i+1]            \n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "def build_strats(D, max_len, V, d, d_demo, N, he, dropout, forecast=False):\n",
    "    start = time.time()\n",
    "    demo = Input(shape=(D,))\n",
    "    demo_enc = Dense(2*d_demo, activation='tanh')(demo)\n",
    "    demo_enc = Dense(d_demo, activation='tanh')(demo_enc)\n",
    "    \n",
    "    varis = Input(shape=(max_len,))\n",
    "    values = Input(shape=(max_len,))\n",
    "    times = Input(shape=(max_len,))\n",
    "    varis_emb = Embedding(V+1, d)(varis)\n",
    "    cve_units = int(np.sqrt(d))\n",
    "    values_emb = CVE(cve_units, d)(values)\n",
    "    times_emb = CVE(cve_units, d)(times)\n",
    "    comb_emb = Add()([varis_emb, values_emb, times_emb]) # b, L, d\n",
    "    mask = Lambda(lambda x:K.clip(x,0,1))(varis) # b, L\n",
    "    cont_emb = Transformer(N, he, dk=None, dv=None, dff=None, dropout=dropout)(comb_emb, mask=mask)\n",
    "    attn_weights = Attention(2*d)(cont_emb, mask=mask)\n",
    "    fused_emb = Lambda(lambda x:K.sum(x[0]*x[1], axis=-2))([cont_emb, attn_weights])\n",
    "    conc = Concatenate(axis=-1)([fused_emb, demo_enc])\n",
    "    encoder_model = Model([demo, times, values, varis], conc)\n",
    "    \n",
    "    if forecast:\n",
    "        fore_op = Dense(V)(conc)\n",
    "        fore_model = Model([demo, times, values, varis], fore_op)\n",
    "        return [encoder_model, fore_model]\n",
    "    \n",
    "    return encoder_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "quick-fetish",
   "metadata": {},
   "source": [
    "## Pretrain on forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-grain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if DATA_SET == 'bodo':\n",
    "    lr, batch_size, samples_per_epoch, patience = 0.0005, 32, 10240, 10\n",
    "\n",
    "elif DATA_SET=='physionet':\n",
    "    lr, batch_size, samples_per_epoch, patience = 0.0005, 32, 10240, 10\n",
    "\n",
    "\n",
    "#########################################################################################################################\n",
    "#########################################################################################################################\n",
    "#NOTE: The weights get saved in the same folder as this code and not the weight folder that is used for the encoder. \n",
    "#NOTE: This is to reduce risk of overwriting  it. \n",
    "#########################################################################################################################\n",
    "#########################################################################################################################\n",
    "\n",
    "\n",
    "# d, N, he, dropout = 50, 2, 4, 0.2\n",
    "d_var = HYPER_PARAM['d_var']  \n",
    "d_demo = HYPER_PARAM['d_demo'] \n",
    "N = HYPER_PARAM['N'] \n",
    "he = HYPER_PARAM['he']  \n",
    "dropout = HYPER_PARAM['dropout'] \n",
    "print(f'D={D}, fore_max_len={fore_max_len}, V={V}, d_var={d_var}, d_demo={d_demo}, N={N}, he={he}, dropout={dropout}')\n",
    "\n",
    "encoder_model, fore_model =  build_strats(D, fore_max_len, V, d_var, d_demo, N, he, dropout, forecast=True)\n",
    "print (fore_model.summary())\n",
    "fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "\n",
    "\n",
    "tf.keras.utils.plot_model(encoder_model, to_file='Images/STraTS_architecture.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7280661",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(1000):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        e_loss += fore_model.train_on_batch([ip[ind] for ip in fore_train_ip], fore_train_op[ind])\n",
    "        pbar.set_description('%f'%(e_loss/(start+1)))\n",
    "    val_loss = fore_model.evaluate(fore_valid_ip, fore_valid_op, batch_size=batch_size, verbose=1)\n",
    "    print ('Epoch', e, 'loss', e_loss*batch_size/samples_per_epoch, 'val loss', val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        #Save weights\n",
    "        fore_model.save_weights(SAVE_WEIGHTS_AS)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch)>patience:\n",
    "        print('break at best_epoch:', best_epoch)\n",
    "        break\n",
    "print(f\"Done! Saved weigts as: {SAVE_WEIGHTS_AS}\")\n",
    "print(\"NOTE: Need to manually move weights to the <weigts> folder to use them further in the clustering_STraTS_encoder script. \\nThis is to make sure that you don't overwrite them.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "b151e83ea101d9f812f847a912c06381d6dfc7e69860785c6e0c5c07911c02ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
